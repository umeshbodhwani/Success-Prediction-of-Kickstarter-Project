{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.051382Z",
     "start_time": "2018-09-17T22:06:26.487771Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "\n",
    "master_df = pd.read_csv('../data/us_kickstarter.zip')\n",
    "df = copy(master_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_select = ['category', 'category_parent_id', 'days_to_deadline', 'staff_pick', \\\n",
    "                 'location_type', 'location_state', 'location_id', 'goal', \\\n",
    "                  'launched_weekday', 'description_length', 'is_weekend_launch', \\\n",
    "                 'launched_week', 'launched_month']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def clean_words (word):\n",
    "    word = word.lower()\n",
    "    word_list = word.split()\n",
    "    clean_words = [word for word in word_list if not word.isdigit()]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "df['cleaned_description'] = df['blurb'].fillna(\"\").apply(clean_words)\n",
    "# df['cleaned_description'] = df['cleaned_description'].apply(stem_description)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 500, use_idf = True, \\\n",
    "                        stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "X = tfidf.fit_transform(df['cleaned_description'].values)\n",
    "\n",
    "vectors = pd.DataFrame(X.toarray(), columns=['_' + w for w in tfidf.get_feature_names()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_select.extend(vectors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['state'] != 'live']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = np.where(df['state'] == 'successful', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['state']\n",
    "df = df[features_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_category = LabelEncoder()\n",
    "le_location_type = LabelEncoder()\n",
    "le_location_state = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = le_category.fit_transform(df['category'])\n",
    "\n",
    "df['location_type'] = le_location_type.fit_transform(df['location_type'])\n",
    "\n",
    "df['location_state'] = le_location_state.fit_transform(df['location_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category_parent_id'] = df['category_parent_id'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_scale = ['goal', 'description_length', 'days_to_deadline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(df, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval, Xtest, yval, ytest = train_test_split(Xtest, ytest, test_size = 0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "to_scale = ['goal', 'description_length', 'days_to_deadline']\n",
    "scaler = StandardScaler()\n",
    "Xtrain.loc[:,to_scale] = scaler.fit_transform(Xtrain[to_scale])\n",
    "Xval.loc[:,to_scale] = scaler.transform(Xval[to_scale])\n",
    "Xtest.loc[:,to_scale] = scaler.transform(Xtest[to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = Xtrain.values\n",
    "Xval = Xval.values\n",
    "Xtest = Xtest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = ytrain.values\n",
    "yval = yval.values\n",
    "ytest = ytest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode labels\n",
    "def one_hot_encode(y_):\n",
    "    z = np.zeros((y_.shape[0], 2))\n",
    "    for i in range(y_.shape[0]):\n",
    "        z[i][y_[i]] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = one_hot_encode(ytrain)\n",
    "\n",
    "yval = one_hot_encode(yval)\n",
    "ytest = one_hot_encode(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.107135Z",
     "start_time": "2018-09-17T22:06:31.090083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "#--------------------------------\n",
    "# learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "#training_epochs = 1000\n",
    "#batch_size = 30\n",
    "\n",
    "training_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "display_step = 2\n",
    "\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Two hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 1000\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 600\n",
    "# number of neurons in layer 3\n",
    "n_hidden_3 = 300\n",
    "# number of neurons in layer 4\n",
    "n_hidden_4 = 100\n",
    "# number of neurons in layer 5\n",
    "n_hidden_5 = 25\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 513\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.158231Z",
     "start_time": "2018-09-17T22:06:31.126508Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    # comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0/weight_shape[0])**0.5\n",
    "    #print(weight_shape[0])\n",
    "    #w_std = 0.5;\n",
    "\n",
    "    #initialization of the weights\n",
    "    #you can try either\n",
    "    w_0 = tf.random_normal_initializer(stddev=w_std)\n",
    "    #w_0 = tf.random_uniform_initializer(minval=-1,maxval=1)\n",
    "\n",
    "    b_0 = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_0)\n",
    "    b = tf.get_variable(\"b\", bias_shape,   initializer=b_0)\n",
    "    \n",
    "    print('Weight Matrix:', W)\n",
    "    print('Bias Vector:', b)\n",
    "    \n",
    "    # different activation functions\n",
    "    # you can try\n",
    "    # (1) linear activation (not a good idea)\n",
    "    #return tf.matmul(x, W) + b\n",
    "    # (2) tanh activation\n",
    "    #return tf.nn.tanh(tf.matmul(x, W) + b)\n",
    "    # (3) sigmoid activation\n",
    "    #return tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "    # (4) relu activation\n",
    "    return tf.nn.sigmoid(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.193619Z",
     "start_time": "2018-09-17T22:06:31.176269Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    \"\"\"\n",
    "    define the whole network (5 hidden layers + output layers)\n",
    "    input:\n",
    "        - a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1])\n",
    "        #print([input_size, n_hidden_1])\n",
    "     \n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "        #print([n_hidden_1, n_hidden_2])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_3\"):\n",
    "        hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3])\n",
    "        #print([n_hidden_2, n_hidden_3])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_4\"):\n",
    "        hidden_4 = layer(hidden_3, [n_hidden_3, n_hidden_4], [n_hidden_4])\n",
    "        #print([n_hidden_3, n_hidden_4])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_5\"):\n",
    "        hidden_5 = layer(hidden_4, [n_hidden_4, n_hidden_5], [n_hidden_5])\n",
    "        #print([n_hidden_4, n_hidden_5])\n",
    "     \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_5, [n_hidden_5, output_size], [output_size])\n",
    "        #print([n_hidden_5, output_size])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define First Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.251904Z",
     "start_time": "2018-09-17T22:06:31.235310Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_1(output, y):\n",
    "    \"\"\"\n",
    "    computes the average error per data sample \n",
    "    by computing the cross-entropy loss over a minibatch\n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    #tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.\n",
    "    xentropy = -tf.reduce_sum(dot_product, 1)\n",
    "    \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T20:17:03.371287Z",
     "start_time": "2018-09-17T20:17:03.367055Z"
    }
   },
   "source": [
    "## Define Second Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.284838Z",
     "start_time": "2018-09-17T22:06:31.276738Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_2(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    #Computes softmax cross entropy between logits and labels.\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.313660Z",
     "start_time": "2018-09-17T22:06:31.305501Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    # tf.train.GradientDescentOptimizer\n",
    "    # will use different optimization routimes \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.556011Z",
     "start_time": "2018-09-17T22:06:31.541378Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:27:56.024948Z",
     "start_time": "2018-09-17T22:27:29.279712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'multi_da_layer/hidden_layer_1/W:0' shape=(513, 1000) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/hidden_layer_1/b:0' shape=(1000,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_da_layer/hidden_layer_2/W:0' shape=(1000, 600) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/hidden_layer_2/b:0' shape=(600,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_da_layer/hidden_layer_3/W:0' shape=(600, 300) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/hidden_layer_3/b:0' shape=(300,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_da_layer/hidden_layer_4/W:0' shape=(300, 100) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/hidden_layer_4/b:0' shape=(100,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_da_layer/hidden_layer_5/W:0' shape=(100, 25) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/hidden_layer_5/b:0' shape=(25,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_da_layer/output/W:0' shape=(25, 2) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_da_layer/output/b:0' shape=(2,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From <ipython-input-27-cbc6c9ace953>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 001 cost function= 0.6874548  Validation Error: 0.44506406784057617\n",
      "Epoch: 003 cost function= 0.6868494  Validation Error: 0.44506406784057617\n",
      "Epoch: 005 cost function= 0.6868529  Validation Error: 0.44506406784057617\n",
      "Epoch: 007 cost function= 0.6868565  Validation Error: 0.44506406784057617\n",
      "Epoch: 009 cost function= 0.6868601  Validation Error: 0.44506406784057617\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.55560637\n",
      "Execution time (seconds) was 181.290\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"multi_da_layer\"):\n",
    "            #neural network definition \n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # image vector & label\n",
    "            x = tf.placeholder(\"float\", [None, input_size])   # MNIST data image of shape 28*28=784\n",
    "            y = tf.placeholder(\"float\", [None, output_size])  # 0-9 digits recognition\n",
    "\n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x)\n",
    "            #calculate loss with tensorflow's cross-entropy function\n",
    "            cost = loss_2(output, y)\n",
    "            \n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            \n",
    "            train_op = training(cost, global_step)\n",
    "            #train_op = training(cost, global_step=None)\n",
    "            \n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op = evaluate(output, y)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "    \n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            #\n",
    "            summary_writer = tf.summary.FileWriter(log_files_path+'multi_da_layer/', sess.graph)\n",
    "        \n",
    "            #initialization of all the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "        \n",
    "            #will work with this later\n",
    "            #saver.restore(sess, log_files_path+'multi_layer/model-checkpoint-66000')\n",
    "            \n",
    "            loss_trace = []\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(Xtrain.shape[0]/batch_size)\n",
    "            \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "\n",
    "                    minibatch_x = Xtrain[i*batch_size: (i+1)*batch_size ]\n",
    "                    minibatch_y = ytrain[i*batch_size: (i+1)*batch_size ]\n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    # Compute average loss of all batches\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "                    \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    #the accuracy is evaluated using the validation dataset\n",
    "                    accuracy = sess.run(eval_op, feed_dict={x: Xval, y: yval})\n",
    "                    loss_trace.append(1-accuracy)    \n",
    "                    print(\"Epoch:\", '%03d' % (epoch+1), \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                        \n",
    "                    #save to use later\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                    #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    saver.save(sess, log_files_path+'multi_da_layer/model-checkpoint', global_step=global_step)\n",
    "                        \n",
    "            print(\"Optimization Finished!\")\n",
    "            #accuracy evaluated with the whole test dataset\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: Xtest, y: ytest})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Execution time (seconds) was %0.3f' % elapsed_time)\n",
    "            \n",
    "            # Visualization of the results\n",
    "            # loss function\n",
    "            #plt.plot(loss_trace)\n",
    "            #plt.title('Cross Entropy Loss')\n",
    "            #plt.xlabel('epoch')\n",
    "            #plt.ylabel('loss')\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
